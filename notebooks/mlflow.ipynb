{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, SimpleRNN, LSTM\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style for better visuals\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "fraud_data = pd.read_csv('../data/fraud_cleaned_data.csv')\n",
    "credit_data = pd.read_csv('../data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((151112, 19), (284807, 31))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data.shape, credit_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['purchase_value', 'age', 'ip_address', 'class', 'frequency', 'velocity',\n",
       "        'hour_of_day', 'day_of_week', 'time_diff', 'signup_hour',\n",
       "        'signup_day_of_week', 'purchase_day_of_week', 'source_Direct',\n",
       "        'source_SEO', 'browser_FireFox', 'browser_IE', 'browser_Opera',\n",
       "        'browser_Safari', 'sex_M'],\n",
       "       dtype='object'),\n",
       " Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "        'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "        'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "        'Class'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data.columns, credit_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Credit Card Data\n",
    "X_creditcard = credit_data.drop(columns=['Class'])  # independant Features\n",
    "y_creditcard = credit_data['Class']                   # Target variable\n",
    "\n",
    "# For Fraud Data\n",
    "X_fraud = fraud_data.drop(columns=['class'])  # independant Features\n",
    "y_fraud = fraud_data['class']      # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for Credit Card Data\n",
    "X_train_creditcard, X_test_creditcard, y_train_creditcard, y_test_creditcard = train_test_split(\n",
    "    X_creditcard, y_creditcard, test_size=0.2, random_state=42, stratify=y_creditcard\n",
    ")\n",
    "\n",
    "# Train-test split for Fraud Data\n",
    "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
    "    X_fraud, y_fraud, test_size=0.2, random_state=42, stratify=y_fraud\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1], dtype=int64), array([109568,  11321], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([227451,    394], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_fraud, return_counts=True))\n",
    "print(np.unique(y_train_creditcard, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/23 22:15:23 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '691f0349eb2843af941b0240a3f5b956', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2024/10/23 22:15:24 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2024/10/23 22:15:24 WARNING mlflow.sklearn: Failed to infer model signature: the trained model does not have a `predict` or `transform` function, which is required in order to infer the signature\n",
      "2024/10/23 22:15:24 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/10/23 22:15:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/10/23 22:15:33 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '029c6dbf25d84676be5afe08561afdaa', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2024/10/23 22:15:33 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2024/10/23 22:15:33 WARNING mlflow.sklearn: Failed to infer model signature: the trained model does not have a `predict` or `transform` function, which is required in order to infer the signature\n",
      "2024/10/23 22:15:33 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/10/23 22:15:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE for Credit Card Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote_creditcard = SMOTE(random_state=42)\n",
    "X_train_creditcard_resampled, y_train_creditcard_resampled = smote_creditcard.fit_resample(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Apply SMOTE for Fraud Data\n",
    "smote_fraud = SMOTE(random_state=42)\n",
    "X_train_fraud_resampled, y_train_fraud_resampled = smote_fraud.fit_resample(X_train_fraud, y_train_fraud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1], dtype=int64), array([109568, 109568], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([227451, 227451], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_fraud_resampled, return_counts=True))\n",
    "print(np.unique(y_train_creditcard_resampled, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraud Data Metrics:\n",
      "Accuracy: 0.6794\n",
      "Precision: 0.8814\n",
      "Recall: 0.6794\n",
      "F1 Score: 0.7458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/23 22:16:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 1d35b9af9f4e4d73bfa790ac9b25f6cc\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "with mlflow.start_run():\n",
    "    # Model training\n",
    "    model = LogisticRegression(max_iter=100)\n",
    "    model.fit(X_train_fraud_resampled, y_train_fraud_resampled)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_fraud = model.predict(X_test_fraud)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy_fraud = accuracy_score(y_test_fraud, y_pred_fraud)\n",
    "    precision_fraud = precision_score(y_test_fraud, y_pred_fraud, average='weighted')\n",
    "    recall_fraud = recall_score(y_test_fraud, y_pred_fraud, average='weighted')\n",
    "    f1_fraud = f1_score(y_test_fraud, y_pred_fraud, average='weighted')\n",
    "\n",
    "   # Display Metrics for Fraud Data\n",
    "    print(\"\\nFraud Data Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy_fraud:.4f}\")\n",
    "    print(f\"Precision: {precision_fraud:.4f}\")\n",
    "    print(f\"Recall: {recall_fraud:.4f}\")\n",
    "    print(f\"F1 Score: {f1_fraud:.4f}\")\n",
    "\n",
    "    # Log parameters and metrics manually (optional)\n",
    "    mlflow.log_metric(\"Accuracy\", accuracy_fraud)\n",
    "    mlflow.log_metric(\"Precision\", precision_fraud)\n",
    "    mlflow.log_metric(\"Recall\", recall_fraud)\n",
    "    mlflow.log_metric(\"F1 Score\", f1_fraud)\n",
    "\n",
    "    # Log the model to MLflow\n",
    "    mlflow.sklearn.log_model(model, \"logistic_regression_model\")\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"max_iter\", 100)\n",
    "    mlflow.log_param(\"solver\", \"lbfgs\")\n",
    "\n",
    "\n",
    "    # Optionally save artifacts (like plots)\n",
    "    # Example: save confusion matrix, ROC curve, etc.\n",
    "    \n",
    "    # Print the run ID for reference\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraud Data Metrics:\n",
      "Accuracy: 0.9775\n",
      "Precision: 0.9982\n",
      "Recall: 0.9775\n",
      "F1 Score: 0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/23 22:16:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 6f93090bd2674e26aa60e57fea39081c\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "with mlflow.start_run():\n",
    "    # Model training\n",
    "    model = LogisticRegression(max_iter=100)\n",
    "    model.fit(X_train_creditcard_resampled, y_train_creditcard_resampled)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_creditcard = model.predict(X_test_creditcard)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy_creditcard = accuracy_score(y_test_creditcard, y_pred_creditcard)\n",
    "    precision_creditcard = precision_score(y_test_creditcard, y_pred_creditcard, average='weighted')\n",
    "    recall_creditcard = recall_score(y_test_creditcard, y_pred_creditcard, average='weighted')\n",
    "    f1_creditcard = f1_score(y_test_creditcard, y_pred_creditcard, average='weighted')\n",
    "\n",
    "   # Display Metrics for Fraud Data\n",
    "    print(\"\\nFraud Data Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy_creditcard:.4f}\")\n",
    "    print(f\"Precision: {precision_creditcard:.4f}\")\n",
    "    print(f\"Recall: {recall_creditcard:.4f}\")\n",
    "    print(f\"F1 Score: {f1_creditcard:.4f}\")\n",
    "\n",
    "    # Log parameters and metrics manually (optional)\n",
    "    mlflow.log_metric(\"Accuracy\", accuracy_creditcard)\n",
    "    mlflow.log_metric(\"Precision\", precision_creditcard)\n",
    "    mlflow.log_metric(\"Recall\", recall_creditcard)\n",
    "    mlflow.log_metric(\"F1 Score\", f1_creditcard)\n",
    "\n",
    "    # Log the model to MLflow\n",
    "    mlflow.sklearn.log_model(model, \"logistic_regression_model\")\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"max_iter\", 100)\n",
    "    mlflow.log_param(\"solver\", \"lbfgs\")\n",
    "\n",
    "\n",
    "    # Optionally save artifacts (like plots)\n",
    "    # Example: save confusion matrix, ROC curve, etc.\n",
    "    \n",
    "    # Print the run ID for reference\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
